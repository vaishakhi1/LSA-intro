{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "857ac754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/vm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/vm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/vm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import functools\n",
    "import operator\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "from num2words import num2words\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "pd.set_option('precision', 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87397ed3",
   "metadata": {},
   "source": [
    "# Import dataset\n",
    "\n",
    "To import the dataset. For now, import using from a csv file. In a later iteration, provide an option to import files. [ To add: An option to read from webpages, different separators]\n",
    "\n",
    "In the example, we are looking at a small collection of short senyences related to two different topics. This should allow us to envision the effects of changes in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b23048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = pd.read_csv('/Users/vm/OneDrive/UNC/lsi/text_corpus.txt', header=None, names=['TEXT'],sep = \"\\n\")\n",
    "# example = example['TEXT'].tolist()\n",
    "\n",
    "example = ['Human machine interface for ABC computer applications',\n",
    "           'A survey of user opinion of computer system response time',\n",
    "           'The EPS user interface management system',\n",
    "           'System and human system engineering testing of EPS',\n",
    "           'Relation of user perceived response time to error measurement',\n",
    "           'The generation of random, binary, ordered trees',\n",
    "           'The intersection graph of paths in trees',\n",
    "           'Graph minor IV: Widths of trees and well-quasi-ordering',\n",
    "           'Graph minor: A survey']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc2f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some Functions\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "def count_tokens(w,example_tok):\n",
    "    count = 0\n",
    "    for sent in example_tok:\n",
    "        if w in sent:\n",
    "            count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a65016",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Before constructing the matrix, the text should be preprocessed. This involved \n",
    "1) retaining the stem of the words by removing plurals, tenses\n",
    "2) converting numbers to words\n",
    "3) removing stop words and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe4a9c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed\n",
      "lemmatized\n",
      "number to worded\n",
      "stopwords removed\n",
      "punctuations removed\n",
      "tokenized\n",
      "34\n",
      "['tree', 'interface', 'system', 'graph', 'computer', 'user', 'human', 'survey', 'eps', 'minor', 'time', 'response']\n"
     ]
    }
   ],
   "source": [
    "## Vectorize\n",
    "#lower case\n",
    "example_LC = np.char.lower(example)\n",
    "\n",
    "#print(example_LC)\n",
    "\n",
    "\n",
    "# Stem\n",
    "example_stem = []\n",
    "#different stemmer algorithms may be used\n",
    "ps = PorterStemmer()\n",
    "for sentence in example_LC:\n",
    "  text = ' '.join([ps.stem(word) for word in sentence.split()])\n",
    "  example_stem.append(text)\n",
    "\n",
    "print('stemmed')\n",
    "\n",
    "\n",
    "# Lemmatize\n",
    "example_lem = []\n",
    "#different stemmer algorithms may be used\n",
    "lm = WordNetLemmatizer()\n",
    "for sentence in example_LC:\n",
    "  text = ' '.join([lm.lemmatize(word) for word in sentence.split()])\n",
    "  example_lem.append(text)\n",
    "\n",
    "print('lemmatized')\n",
    "\n",
    "\n",
    "# Number to words\n",
    "example_num = []\n",
    "for sentence in example_lem:\n",
    "  text = ' '.join([num2words(word) if word.isnumeric() else word for word in sentence.split()])\n",
    "  example_num.append(text)\n",
    "print('number to worded')\n",
    "    \n",
    "\n",
    "#remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "example_stop = []\n",
    "for sentence in example_num:\n",
    "  text = ' '.join([word for word in sentence.split() if word not in stopwords_dict])\n",
    "  example_stop.append(text)\n",
    "print('stopwords removed')\n",
    "\n",
    "\n",
    "# remove punctuations\n",
    "symbols = \"!\\\"#$%&()*+-.,/:;<=>?@[\\]^_`{|}~\\n\"\n",
    "for i in symbols:\n",
    "    example_stop = np.char.replace(example_stop, i, ' ')\n",
    "print('punctuations removed')\n",
    "\n",
    "# print(example_stop)\n",
    "\n",
    "example_tok = []\n",
    "for sentence in example_stop:\n",
    "  example_tok.append(nltk.word_tokenize(sentence))\n",
    "\n",
    "\n",
    "print('tokenized')\n",
    "\n",
    "\n",
    "# Prepare a list of tokens that are repeated atleast once\n",
    "token_list = flatten(example_tok)\n",
    "token_list = list(set(token_list))\n",
    "print(len(token_list))\n",
    "new_list = []\n",
    "for word in token_list: \n",
    "    if count_tokens(word,example_tok)>1:\n",
    "        new_list.append(word)\n",
    "print(new_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066ec30",
   "metadata": {},
   "source": [
    "# Constructing the TF-IDF matrix \n",
    "\n",
    "The term frequency for each term $tf(w,d)$ is the frequency of the word $w$ in document $d$. Document frequency $df(w)$ is the number of documents the word $w$ appears in. If a word appears in too many documents, it is a common word and does not help with understanding how the documents are different. Therefore, the term frequency matrix is weighted by the inverse of the document frequency.\n",
    "\n",
    "examples of term-document matrices\n",
    "\n",
    "Only term frequency\n",
    "$$T[w,d] = tf[w,d]$$\n",
    "\n",
    "TF-IDF\n",
    "$$TD = tf[w,d]*\\log(N_t/df[w])$$\n",
    "\n",
    "Other term document matricies\n",
    "\n",
    "#A bit more about TF-IDF and Vector space Models\n",
    "VSMs (1970, Salton) and TF-IDF (1975, Salton et.. al)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09996905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "12\n",
      "34\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vm/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############################################\n",
    "\n",
    "N_d = len(example_tok)\n",
    "print(N_d)\n",
    "\n",
    "#termlist =  functools.reduce(operator.iconcat, example_tok, [])\n",
    "#termlist = list(set(termlist))\n",
    "\n",
    "N_t = len(new_list)\n",
    "tf_mat = np.zeros(shape = (N_d,N_t))\n",
    "tf_mat = csr_matrix(tf_mat)\n",
    "tf_idf = csr_matrix(tf_mat)\n",
    "\n",
    "# Document frequency\n",
    "DF = {}\n",
    "for i in range(N_d):\n",
    "    tokens = example_tok[i]\n",
    "    for w in tokens:\n",
    "        if w in new_list:\n",
    "            try:\n",
    "                DF[w].add(i)\n",
    "            except:\n",
    "                DF[w] = {i}\n",
    "\n",
    "\n",
    "for i in DF:\n",
    "  DF[i] = len(DF[i])\n",
    "DF = np.fromiter(DF.values(), dtype=float) \n",
    "\n",
    "idf = np.log(N_t/DF) +1\n",
    "\n",
    "\n",
    "\n",
    "# print(new_list)\n",
    "\n",
    "\n",
    "#term frequency and tfidf\n",
    "for i in range(N_d):\n",
    "    # i = 3\n",
    "    print(i)\n",
    "    tokens = example_tok[i]\n",
    "    #print(tokens)\n",
    "\n",
    "    words_count = len(example_tok[i])\n",
    "    counter = Counter(example_tok[i])\n",
    "\n",
    "    for token in new_list:\n",
    "        try:\n",
    "            ind = new_list.index(token)\n",
    "\n",
    "            tf = counter[token]/words_count\n",
    "            tf2 = counter[token]\n",
    "\n",
    "            tf_mat[i,ind] = tf2\n",
    "            tf_idf[i,ind] = tf*idf[ind]\n",
    "            #df1 = DF[token]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "tf_mat_df = pd.DataFrame(tf_mat.toarray(),columns = new_list)\n",
    "tf_idf_df = pd.DataFrame(tf_idf.toarray(),columns = new_list)\n",
    "#print(tf_idf_df)\n",
    "tf_mat = tf_mat.toarray()\n",
    "from scipy import stats\n",
    "print(len(new_list))\n",
    "print(len(token_list))\n",
    "print(N_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727a5ed",
   "metadata": {},
   "source": [
    "# LSA\n",
    "\n",
    "Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73090bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05591352  0.1973928 ]\n",
      " [-0.16559288  0.60599027]\n",
      " [ 0.12731206  0.46291751]\n",
      " [ 0.23175523  0.54211442]\n",
      " [-0.10677472  0.27946911]\n",
      " [-0.19284794  0.00381521]\n",
      " [-0.43787488  0.01463147]\n",
      " [-0.6151219   0.02413684]\n",
      " [-0.52993707  0.08195737]]\n",
      "[0 0 0 0 0 1 1 1 1]\n",
      "   tree  interface  system  graph  computer  user  human  survey   eps  minor  \\\n",
      "0 -0.06       0.14    0.45  -0.06      0.15  0.26   0.16    0.10  0.22  -0.04   \n",
      "1  0.23       0.37    1.23   0.34      0.51  0.84   0.40    0.53  0.55   0.25   \n",
      "2 -0.14       0.33    1.05  -0.15      0.36  0.61   0.38    0.23  0.51  -0.10   \n",
      "3 -0.27       0.40    1.27  -0.30      0.41  0.70   0.47    0.21  0.63  -0.21   \n",
      "4  0.14       0.16    0.56   0.20      0.24  0.39   0.18    0.27  0.24   0.15   \n",
      "5  0.24      -0.03   -0.07   0.31      0.02  0.03  -0.05    0.14 -0.07   0.22   \n",
      "6  0.55      -0.07   -0.15   0.69      0.06  0.08  -0.12    0.31 -0.14   0.50   \n",
      "7  0.77      -0.10   -0.21   0.98      0.09  0.12  -0.16    0.44 -0.20   0.71   \n",
      "8  0.66      -0.04   -0.05   0.85      0.12  0.19  -0.09    0.42 -0.11   0.62   \n",
      "\n",
      "   time  response  \n",
      "0  0.16      0.16  \n",
      "1  0.58      0.58  \n",
      "2  0.38      0.38  \n",
      "3  0.42      0.42  \n",
      "4  0.28      0.28  \n",
      "5  0.06      0.06  \n",
      "6  0.13      0.13  \n",
      "7  0.19      0.19  \n",
      "8  0.22      0.22  \n",
      "0.9166666666666666\n",
      "SpearmanrResult(correlation=0.9166666666666666, pvalue=0.0005066190663052084)\n",
      "SpearmanrResult(correlation=0.8333333333333333, pvalue=0.005265691029161748)\n"
     ]
    }
   ],
   "source": [
    "k =2\n",
    "U,s,Vt = svds(tf_mat,k=k)\n",
    "# print(np.transpose(Vt))\n",
    "print(U)\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(U)\n",
    "print(kmeans.labels_)\n",
    "# plt.scatter(U[:,0],U[:,4],U[:,5],c=kmeans.labels_.astype(float))\n",
    "\n",
    "tf_hat = np.matmul(np.matmul(U,np.diag(s)),Vt)\n",
    "print(pd.DataFrame(tf_hat, columns = new_list))\n",
    "y, rho =stats.spearmanr(tf_hat[:,2], tf_hat[:,4])\n",
    "print(y)\n",
    "print(stats.spearmanr(tf_hat[:,2], tf_hat[:,4]))\n",
    "print(stats.spearmanr(tf_hat[:,2], tf_hat[:,10]))\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(U)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c20a6",
   "metadata": {},
   "source": [
    "# SVD Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c34d0",
   "metadata": {},
   "source": [
    "# SVD Standard \n",
    "\n",
    "TF matrix variants\n",
    "1) standard term frequency. Leads to bias towards documents with many term occurrences.\n",
    "\n",
    "2) TF sum - The term frequencies are normalized by the total number of terms in the document. Leads to biases towards short documents\n",
    "$$TF_{\\text{sum}} = \\frac{tf_{L}[w,d]}{\\sum_w{tf_{L}[w,d]}}$$\n",
    "\n",
    "The biases induced by variants 1 and 2 cannot be observed in the example being considered, since all documents are of equal length and have roughly the same number of terms.\n",
    "\n",
    "3) TF max - The term frequencies are normalized by the highest number of occrrences for any term in the document. Mitigates the biases of variants 1 and 2\n",
    "$$TF_{\\text{max}} = \\frac{tf_{L}[w,d]}{\\max_w{tf_{L}[w,d]}}$$\n",
    "\n",
    "4) TF log - reduces the impact of each additional occurrence\n",
    "$$TF_{\\text{log}} = \\log(1+ tf_{L}[w,d])$$\n",
    "\n",
    "5) TF frac - Here $K_d$ is the pivoted document length. If $L_D = \\text{avg}({\\ell_d})$, where $\\ell_d$ is the length of document $d$, the pivoted document length is given by $K_d = \\ell_d/L_D$. It is less than one for short documents and more than one for longer documents. Like the logarithmic approach, the effect of each additional additional occurrence is reduced, the reduction is more pronounced for larger documents\n",
    "\n",
    "\n",
    "$$TF_{\\text{frac,K}} = \\frac{tf_{L}[w,d]}{tf_{L}[w,d] + K_d}$$\n",
    "\n",
    "The graph in the next cell shows the comparison between the standard, log and the fractional version of TF (for $K_d = 0.5,2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a012b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing the effect of different \n",
    "x = np.array(range(10))\n",
    "print(np.log(x))\n",
    "plt.plot(x,x)\n",
    "plt.plot(x,x/(x+2))\n",
    "plt.plot(x,x/(x+0.5))\n",
    "plt.plot(x, np.log(x))\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('TF')\n",
    "plt.legend(['TF std','K_d = 0.5','K_d = 2','log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "146879b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.846  1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 1.     0.846  1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 1.     0.846  1.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.719  0.972  0.719  0.719  1.     0.     0.     0.     0.   ]\n",
      " [-0.839 -0.558 -0.839 -0.839 -0.389  1.     0.     0.     0.   ]\n",
      " [-0.839 -0.558 -0.839 -0.839 -0.389  1.     1.     0.     0.   ]\n",
      " [-0.839 -0.558 -0.839 -0.839 -0.389  1.     1.     1.     0.   ]\n",
      " [-0.804 -0.481 -0.804 -0.804 -0.298  0.979  0.979  0.979  1.   ]]\n",
      "   tree  interface  system  graph  computer  user  human  survey  eps  minor  \\\n",
      "0   0.0        1.0     0.0    0.0       1.0   0.0    1.0     0.0  0.0    0.0   \n",
      "1   0.0        0.0     1.0    0.0       1.0   1.0    0.0     1.0  0.0    0.0   \n",
      "2   0.0        1.0     1.0    0.0       0.0   1.0    0.0     0.0  1.0    0.0   \n",
      "3   0.0        0.0     2.0    0.0       0.0   0.0    1.0     0.0  1.0    0.0   \n",
      "4   0.0        0.0     0.0    0.0       0.0   1.0    0.0     0.0  0.0    0.0   \n",
      "5   1.0        0.0     0.0    0.0       0.0   0.0    0.0     0.0  0.0    0.0   \n",
      "6   1.0        0.0     0.0    1.0       0.0   0.0    0.0     0.0  0.0    0.0   \n",
      "7   1.0        0.0     0.0    1.0       0.0   0.0    0.0     0.0  0.0    1.0   \n",
      "8   0.0        0.0     0.0    1.0       0.0   0.0    0.0     1.0  0.0    1.0   \n",
      "\n",
      "   time  response  \n",
      "0   0.0       0.0  \n",
      "1   1.0       1.0  \n",
      "2   0.0       0.0  \n",
      "3   0.0       0.0  \n",
      "4   1.0       1.0  \n",
      "5   0.0       0.0  \n",
      "6   0.0       0.0  \n",
      "7   0.0       0.0  \n",
      "8   0.0       0.0  \n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "U,s,Vt = svds(tf_mat,k=k)\n",
    "# plt.scatter(U[:,0],U[:,1],c=kmeans.labels_.astype(float))\n",
    "np.set_printoptions(precision=3)\n",
    "tf_hat = np.matmul(np.matmul(U,np.diag(s)),Vt)\n",
    "# print(pd.DataFrame(tf_hat, columns = new_list))\n",
    "y, rho =stats.spearmanr(tf_hat[:,2], tf_hat[:,4])\n",
    "print(y)\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(U)\n",
    "\n",
    "corr_raw = np.zeros([N_d,N_d])\n",
    "for i in range(N_d):\n",
    "    for j in range(i+1):\n",
    "        t,r = stats.spearmanr(tf_hat[i,:], tf_hat[j,:])\n",
    "        corr_raw[i][j] = t\n",
    "corr = np.zeros([N_d,N_d])\n",
    "for i in range(N_d):\n",
    "    for j in range(i+1):\n",
    "        t,r = stats.spearmanr(tf_mat[i,:], tf_mat[j,:])\n",
    "        corr[i][j] = t\n",
    "print(corr_raw)\n",
    "print(tf_mat_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02853c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.706  1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 1.     0.706  1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 1.     0.706  1.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.937  0.874  0.937  0.937  1.     0.     0.     0.     0.   ]\n",
      " [-0.944 -0.552 -0.944 -0.944 -0.825  1.     0.     0.     0.   ]\n",
      " [-0.944 -0.552 -0.944 -0.944 -0.825  1.     1.     0.     0.   ]\n",
      " [-0.944 -0.552 -0.944 -0.944 -0.825  1.     1.     1.     0.   ]\n",
      " [-0.881 -0.476 -0.881 -0.881 -0.762  0.951  0.951  0.951  1.   ]]\n",
      "   tree  interface  system  graph  computer  user  human  survey   eps  minor  \\\n",
      "0  0.00       0.47    0.00   0.00      0.40  0.00   0.47    0.00  0.00    0.0   \n",
      "1  0.00       0.00    0.40   0.00      0.34  0.34   0.00    0.40  0.00    0.0   \n",
      "2  0.00       0.56    0.56   0.00      0.00  0.48   0.00    0.00  0.56    0.0   \n",
      "3  0.00       0.00    0.93   0.00      0.00  0.00   0.47    0.00  0.47    0.0   \n",
      "4  0.00       0.00    0.00   0.00      0.00  0.34   0.00    0.00  0.00    0.0   \n",
      "5  0.56       0.00    0.00   0.00      0.00  0.00   0.00    0.00  0.00    0.0   \n",
      "6  0.70       0.00    0.00   0.70      0.00  0.00   0.00    0.00  0.00    0.0   \n",
      "7  0.35       0.00    0.00   0.35      0.00  0.00   0.00    0.00  0.00    0.3   \n",
      "8  0.00       0.00    0.00   0.93      0.00  0.00   0.00    0.93  0.00    0.8   \n",
      "\n",
      "   time  response  \n",
      "0  0.00       0.0  \n",
      "1  0.34       0.4  \n",
      "2  0.00       0.0  \n",
      "3  0.00       0.0  \n",
      "4  0.34       0.4  \n",
      "5  0.00       0.0  \n",
      "6  0.00       0.0  \n",
      "7  0.00       0.0  \n",
      "8  0.00       0.0  \n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "U,s,Vt = svds(tf_idf,k=k)\n",
    "# plt.scatter(U[:,0],U[:,1],c=kmeans.labels_.astype(float))\n",
    "np.set_printoptions(precision=3)\n",
    "tf_hat = np.matmul(np.matmul(U,np.diag(s)),Vt)\n",
    "# print(pd.DataFrame(tf_hat, columns = new_list))\n",
    "y, rho =stats.spearmanr(tf_hat[:,2], tf_hat[:,4])\n",
    "print(y)\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(U)\n",
    "\n",
    "corr_raw = np.zeros([N_d,N_d])\n",
    "for i in range(N_d):\n",
    "    for j in range(i+1):\n",
    "        t,r = stats.spearmanr(tf_hat[i,:], tf_hat[j,:])\n",
    "        corr_raw[i][j] = t\n",
    "corr = np.zeros([N_d,N_d])\n",
    "for i in range(N_d):\n",
    "    for j in range(i+1):\n",
    "        t,r = stats.spearmanr(tf_idf[i,:], tf_idf[j,:])\n",
    "        corr[i][j] = t\n",
    "print(corr_raw)\n",
    "print(tf_idf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d2f4a",
   "metadata": {},
   "source": [
    "# LSA paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82233977",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_raw = np.zeros([N_d,N_d])\n",
    "for i in range(N_d):\n",
    "    for j in range(i+1):\n",
    "        t,r = stats.spearmanr(tf_hat[i,:], tf_hat[j,:])\n",
    "        corr_raw[i][j] = t\n",
    "corr = np.zeros([N_d,N_d])\n",
    "for i in range(N_d):\n",
    "    for j in range(i+1):\n",
    "        t,r = stats.spearmanr(tf_mat[i,:], tf_mat[j,:])\n",
    "        corr[i][j] = t\n",
    "print(corr_raw)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfabeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
